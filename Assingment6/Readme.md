
# Stream CSV Domains 📧

A high-performance Node.js project for aggregating email domain counts from a CSV file using **Node.js streams** and providing a simple REST API interface. It also features a custom logging system with file transports and rotation.

-----

## 🚀 Features

  * **Stream-Based Processing**: Efficiently reads a large CSV file line-by-line using Node.js streams, minimizing memory usage.
  * **Domain Aggregation**: Counts the occurrences of each unique email domain.
  * **REST API**: Provides HTTP endpoints to trigger the aggregation process and retrieve the results.
  * **Custom Logging**: Implements a robust, custom `EventEmitter`-based logging system with **console** and **rotating file** transports.
  * **Modular Design**: Separates server logic, processing logic, and logging components.

-----

## 📁 Project Structure

| File | Description |
| :--- | :--- |
| `src/server.mjs` | The main HTTP server application. |
| `src/processDomains.js` | The core logic for reading the CSV, counting domains, and writing the JSON output. |
| `src/streams/LineSplitter.js` | A custom transform stream to split incoming chunks into lines. |
| `src/streams/DomainCounter.js` | A custom transform stream to extract and count domains from lines. |
| `src/logger/*` | Custom logging system components (Logger, Transports). |
| `package.json` | Project metadata and scripts. |
| `Eventemitter.js` | A separate example file demonstrating the custom logging system with file rotation. |
| `data/users.csv` | *(Assumed)* The input CSV file containing user data (e.g., email addresses). |
| `out/domains.json` | *(Output)* The aggregated domain counts in JSON format. |
| `logs/app.log` | *(Output)* Log file generated by the `processDomains` script. |

-----

## 🛠️ Key Code Explanations

### `package.json`

This file defines the project as a **module** (`"type": "module"`) and provides two main utility scripts:

  * `"start"`: Runs the HTTP server using `node src/server.mjs`.
  * `"process"`: Executes the domain aggregation script directly, useful for CLI-only processing or testing.

### `src/server.mjs` (HTTP Server)

The server is built with Node's native `node:http` module.

  * **Initialization**: Sets up the server to listen on port `3000` (or the one specified by the `PORT` environment variable).
  * **`POST /process`**:
      * Dynamically `import`s the `processDomains` function.
      * Calls `await processDomains()` to start the aggregation.
      * Returns a `200 OK` status with `{"ok": true}` on success, or a `500 Internal Server Error` on failure, including the error message.
  * **`GET /domains`**:
      * Checks if the output file (`out/domains.json`) exists.
      * If found, it returns the JSON file content using `fs.createReadStream(file).pipe(res)` for efficient streaming.
      * If not found, it returns a `404 Not found`.

### `src/processDomains.js` (Core Logic)

This is where the streaming magic happens, utilizing the `node:stream/promises` `pipeline` for robust stream management.

1.  **Logging Setup**: Initializes the custom `Logger` with a `ConsoleTransport` and a **rotating `FileTransport`** (`maxBytes: 50 * 1024`, `maxFiles: 5`).
2.  **`processDomains` Function**:
      * Creates the input read stream (`src = fs.createReadStream(input)`).
      * **Custom Streams**: Uses `LineSplitter` and `DomainCounter`. The `DomainCounter` is the key stream that performs the aggregation, storing results in its internal `counts` map.
      * **Pipeline Execution**:
        ```javascript
        await pipeline(src, splitter, counter, passthrough, sink);
        ```
        The data flows from: `CSV File` -\> `LineSplitter` -\> `DomainCounter` -\> `Passthrough` -\> `Writable Sink` (which discards the final data since the result is in `counter.counts`).
      * **JSON Output**: After the pipeline finishes, the script *streams* the final aggregated `counter.counts` map directly to the output file (`out/domains.json`) as a properly formatted JSON object. This is done manually with `out.write()` calls to avoid loading the entire final object into memory if the counts are vast.

### `Eventemitter.js` (Logger Example)

This file serves as a demonstration and test for the custom logging system:

  * It creates a `Logger` instance with both `ConsoleTransport` and `FileTransport`.
  * The `FileTransport` is configured for **file rotation**: it will keep a maximum of 5 log files, each up to 50KB.
  * It logs various levels (`info`, `warn`, `error`, `debug`).
  * It includes a loop that generates **1000 log entries** to specifically trigger and demonstrate the file rotation mechanism.
  * Finally, it calls `logger.close()` to ensure all buffered file writes are completed before the process exits.
